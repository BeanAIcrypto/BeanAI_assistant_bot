import os
import logging
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

from src.services.count_token import count_output_tokens
from db.dbworker import get_user_limit, update_user_limit, get_user_history
from src.generated_answer.agent.agent_response import knowledge_base_search
from src.bot.promt import PROMTS


logger = logging.getLogger(__name__)
load_dotenv()
OPENAI_API_KEY = os.getenv("GPT_SECRET_KEY_FASOLKAAI")

llm = ChatOpenAI(
    model_name="gpt-4o-mini",
    openai_api_key=OPENAI_API_KEY
)

async def is_crypto_related(question: str, user_id: int) -> bool:
    logger.debug(f"[is_crypto_related] –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–ø—Ä–æ—Å: '{question}' –¥–ª—è user_id={user_id}")

    knowledge_snippets = knowledge_base_search(question)
    has_knowledge = knowledge_snippets and "No search results found" not in knowledge_snippets

    if has_knowledge:
        logger.info(f"[is_crypto_related] –í–æ–ø—Ä–æ—Å '{question}' –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã–π.")
        return True

    system_prompt = PROMTS["system_prompt"]
    user_prompt = f"{PROMTS['user_prompt']} {question}\n\nReply with 'True' if the question is directly related to the topic of cryptocurrencies, otherwise reply with 'False'."

    total_tokens = count_output_tokens(system_prompt + user_prompt) + 4
    limit = get_user_limit(user_id)

    if limit - total_tokens < 0:
        logger.warning(f"[is_crypto_related] –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤. –û—Å—Ç–∞—Ç–æ–∫: {limit}, –Ω—É–∂–Ω–æ: {total_tokens}")
        return False

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]

    response = llm(messages)
    update_user_limit(user_id, limit - total_tokens)

    model_answer = response.content.strip()
    logger.debug(f"[is_crypto_related] –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: '{model_answer}'")

    return model_answer == "True"


async def context_completion(question: str, user_id: int) -> str:
    """
    –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å, –µ—Å–ª–∏ –æ–Ω —è–≤–Ω–æ —Å—Å—ã–ª–∞–µ—Ç—Å—è
    –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞. –ï—Å–ª–∏ –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    –≤—Å—ë –ø–æ–Ω—è—Ç–Ω–æ, –≤–µ—Ä–Ω—É—Ç—å –≤–æ–ø—Ä–æ—Å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.
    """
    logger.debug(f"[context_completion] –ü–æ–ª—É—á–µ–Ω –≤–æ–ø—Ä–æ—Å: '{question}' –¥–ª—è user_id={user_id}")

    history = get_user_history(user_id)
    if not history:
        logger.debug("[context_completion] –ò—Å—Ç–æ—Ä–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.")
        return question

    history_list = []
    for entry in history:
        if "question" in entry and "response" in entry:
            history_list.append({
                "user": entry["question"],
                "assistant": entry["response"]
            })
        else:
            logger.warning(f"[context_completion] –ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∑–∞–ø–∏—Å—å –∏—Å—Ç–æ—Ä–∏–∏: {entry}")

    logger.debug(f"[context_completion] –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(history_list)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏.")

    system_prompt = (
        """You are an assistant that improves and clarifies user questions, making them fully self-contained and understandable without the context of the current dialogue.

        Workflow:
        1. Analyze the user's latest message along with previous messages in the conversation.
        2. Identify any ambiguous pronouns (such as: "this", "that", "such", "he", "she", "they", etc.) or references that only make sense within the conversation's context.
        3. Replace each such pronoun or vague reference with specific entities (e.g., names of objects, terms, people, or concepts) explicitly mentioned earlier in the dialogue.
        4. If the user's message includes unfamiliar terms or unclear wording, use the knowledge base to interpret their meaning and incorporate them accurately into the reformulated question.
        5. Do not change the original intent or meaning of the user's question. Your version should stay as close as possible to the original wording, but be fully clear and comprehensible on its own.
        6. If the user's message is already fully clear and self-contained without requiring pronoun replacement or clarification, return it unchanged.

        Your output should be a precise, self-sufficient question that can be fully understood by you and other GPT models without any additional context.
        """
    )

    history_text = ""
    for i, dialog in enumerate(history_list):
        user_text = dialog["user"]
        assistant_text = dialog["assistant"]
        history_text += (
            f"User question {i + 1}: {user_text}\n"
            f"Assistant response {i + 1}: {assistant_text}\n\n"
        )

        knowledge_snippets = knowledge_base_search(question)
        if knowledge_snippets and "No search results found" not in knowledge_snippets:
            knowledge_block = f"üìö Here are relevant excerpts from the knowledge base:\n{knowledge_snippets}\n\n"
        else:
            knowledge_block = ""

    user_prompt = (
        f"Knowledge base content: {knowledge_block}"
        f"Dialogue history:\n{history_text}\n"
        f"New question: '{question}'\n\n"
        "Return either the original question or a rephrased version of it with no references to prior context."
    )

    total_tokens = count_output_tokens(system_prompt + user_prompt) + 4
    limit = get_user_limit(user_id)

    logger.debug(f"[context_completion] –ü–æ–¥—Å—á–∏—Ç–∞–Ω–æ total_tokens={total_tokens}, –¥–æ—Å—Ç—É–ø–Ω–æ limit={limit}")

    if limit - total_tokens < 0:
        logger.warning(
            f"[context_completion] –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤. –û—Å—Ç–∞—Ç–æ–∫: {limit}, –Ω—É–∂–Ω–æ: {total_tokens}. "
            "–í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å."
        )
        return question

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]

    response = llm(messages)
    new_limit = limit - total_tokens
    update_user_limit(user_id, new_limit)

    revised_question = response.content.strip()
    logger.debug(f"[context_completion] –ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å: '{revised_question}'")

    return revised_question
